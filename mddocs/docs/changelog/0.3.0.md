# 0.3.0 (2025-07-04)

## Features

- Improved support for `openlineage-airflow` ({issue}`210`).

  Before we tracked only DAG and Task start/stop events, but not lineage.
  Now we store lineage produces by Airflow Operators like `SQLExecuteQueryOperator`.

- Added support for `openlineage-flink` ({issue}`214`).

- Added support for `openlineage-hive` ({issue}`245`).

- Added support for `openlineage-dbt` ({issue}`223`).

- Add `DATASET` granularity for `GET /api/datasets/lineage` ({issue}`235`).

- Store SQL queries received from OpenLineage integrations. ({issue}`213`, {issue}`218`).

## Breaking changes

- Change `Output.type` in `GET /api/*/lineage` response from `Enum` to `List[Enum]` ({issue}`222`).

  ```{eval-rst}
  .. dropdown:: Response examples

    Before:

    .. code:: python

        {
            "nodes": {...},
            "relations": {
                "outputs": [
                    {
                    "from": {"kind": "JOB", "id": 3981},
                    "to": {"kind": "DATASET", "id": 8400},
                    "types": "OVERWRITE",  # <---
                    ...
                ]
            },
        }

    After

    .. code:: python

        {
            "nodes": {...},
            "relations": {
                "outputs": [
                    {
                    "from": {"kind": "JOB", "id": 3981},
                    "to": {"kind": "DATASET", "id": 8400},
                    "types": ["OVERWRITE", "DROP", "TRUNCATE"],  # <---
                    ...
                ]
            },
        }

    We're using output schema, if any, then fallback to input schema.
  ```

- Moved `Input.schema` and `Output.schema` to `Dataset.schema` in `GET /api/*/lineage` response ({issue}`249`).

  ```{eval-rst}
  .. dropdown:: Response examples

      Before:

      .. code:: python

        {
            "nodes": {
                "datasets": {
                    "8400": {
                        "id": "8400",
                        "location": {...},
                        "name": "dataset_name",
                        ...
                }

            },
            "relations": {
                "outputs": [
                    {
                    "from": {"kind": "JOB", "id": 3981},
                    "to": {"kind": "DATASET", "id": 8400},
                    "types": "OVERWRITE",
                    "schema": {  # <---
                        "id": "10062",
                        "fields": [ ... ],
                        "relevance_type": "EXACT_MATCH"
                    ]
                ]
            },
        }

      After:

      .. code:: python

        {
            "nodes": {
                "datasets": {
                    "8400": {
                        "id": "25896",
                        "location": {...},
                        "name": "dataset_name",
                        "schema": {  # <---
                            "id": "10062",
                            "fields": [...],
                            "relevance_type": "EXACT_MATCH"
                        },
                        ...
                    }
                }
                ...
            },
            "relations": {
                "outputs": [
                    {
                    "from": {"kind": "JOB", "id": 3981},
                    "to": {"kind": "DATASET", "id": 8400},
                    "types": ["OVERWRITE", "DROP", "TRUNCATE"],
                    ...
                ]
            },
        }
  ```

## Improvements

- Added `cleanup_partitions.py` script to automate the cleanup of old table partitions ({issue}`254`).

- Added `data_rentgen.db.seed` script which creates example data in database ({issue}`257`).

- Speedup fetching `Run` and `Operation` from database by id ({issue}`247`).

- Speedup consuming OpenLineage events from Kafka ({issue}`236`).

- Make consumer message parsing more robust ({issue}`204`).

  Previously malformed OpenLineage events (JSON) lead to skipping the entire message batch read from Kafka.
  Now messages are parsed separately, and malformed ones are send back to `input.runs__malformed` Kafka topic.

- Improve storing lineage data for long running operations ({issue}`253`).

  ```{eval-rst}
  .. dropdown:: Description

    Previously if operation was running for a long time (more than a day, Flink streaming jobs can easily run for months or years),
    and lineage graph was build for last day, there were no Flink job/run/operation in the graph.

    This is because we created input/output/column lineage at operation start,
    and ``RUNNING`` events of the same operation (checkpoints) were just updating the same row statistics.

    Now we create new input/output/column lineage row for checkpoints events as well.
    But only one row for each hour since operation was started, as increasing number of rows slows down lineage graph resolution.

    For short-lived operations (most of batch operations take less than hour) behavior remains unchanged.
  ```

## Bug Fixes

- Fix Airflow 3.x DAG and Task url template ({issue}`227`).
